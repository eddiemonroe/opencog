
                       La Cogita Chatbot
                       -----------------
                Linas Vepstas <linasvepstas@gmail.com>
                          August 2009

A quick-n-dirty hookup of the OpenCog NLP pipeline to an IRC chatbot.

At this time, the bot is a demo of the OpenCog natural-language
processing (NLP) pipeline; it is NOT -- repeat NOT -- a demo of
OpenCog reasoning, deduction, or anything like that.

To make the chatbot to actually answer the question with a correct
answer PLN inference should be used and that would be the next
implementation step.

The chatbot can currently answer certain simple English-language
wh-questions.  It does this by using "fuzzy-pattern-matching". This
works by performing a fuzzy search, comparing the question to sentences
in the atomspace: if a similar sentence is found, the fuzzy matcher
will offer that up as an answer.  See below for additional details.

Running the chatbot requires three steps:

1) Running the parser server.
2) Running the opencog server.
3) Running the chat-to-opencog bridge.

Run the parser server:
----------------------
The chatbot requires the English parser daemon to be running.  The
parser is available in the newer versions of RelEx, and is started by
running ./opencog-server.sh from the main RelEx directory. (You must
compile RelEx first, by installing java and ant, and saying "ant" to
build RelEx).

The parse server listens on port 4444: it will read plain-text input and
generates parsed text in OpenCog format. It uses RelEx2Logic rules
for mapping the output of RelEx into a style of OpenCog Atom representation
which will be  used for automated reasoning by PLN. The speech act of the
input sentence will be tagged by R2L.

The chatbot needs this server to process the input text.

There is a simple "sniff-test" that can be used to verify that the
server is running: try "telnet localhost 4444" to connect to the server,
then type in an English sentence, then hit return. It should respond
with a verbose parse of the sentence.

Run the OpenCog server:
-----------------------
The chatbot requires an opencog server running on port 17004. This
server needs to be appropriately configured and loaded with assorted
databases, code, etc.

This is most easily done by starting the cog server using the
"lib/opencog-chatbot.conf" configuration file, which sets the correct
port, silences the OpenCog prompt, and loads the needed scheme files.

Configure and run the chat bridge:
----------------------------------
The chatbot itself is just a simple deamon that bridges text from the
chat system to the OpenCog server. The cogita chatbot is tailored for
IRC chat; other chat systems could be supported in a straightforward
manner.  See the file ../irc/README for details.

Architecture:
-------------
The current architecture makes use of several servers interconnected by
TCP/IP sockets to perform NLP processing.

The pipeline is as follows:

1) IRC I/O, performed by cogita, acts as an intermediary between
   IRC and OpenCog.  It listens for input on an IRC channel, and
   forwards the resulting plain-text to OpenCog. This is done by
   issuing one simple scheme expression to OpenCog, via the OpenCog
   command-line interface/scheme shell on port 17004. The expression
   is ''(process_query user text)'', where the ''user'' is
   the user's IRC nick, and ''text'' is what the user entered.  The
   return value from this command is sent back to the IRC channel.
   Communications is stateless and blocking: cogita closes the socket
   to indicate end-of-messsage, and expects that the cog server will
   do the same. Only after the cog-server closes its socket does cogita
   reply on the IRC channel.

   Note that if the cog-server is busy, then "whirr" can block for an
   indefinitely long time. The person who is chatting will start to
   wonder about the lack of response. This is currently hacked around
   by having the chat processing periodically return to the bridge,
   and then resume again. A proper architecture remains unimplemented.

2) Parsing of English text by RelEx. The very first thing that the
   ''process_query'' routine does is to send the input text to RelEx
   for parsing.  This is done by sending the plain-text via a socket
   to a listening RelEx server; the server responds with the parsed
   text, in the form of a hypergraph of atoms expressed in scheme.
   The hypergraph is then loaded into the OpenCog atomspace, and is
   ready for processing.

   I/O to parser is stateless: RelEx will close the socket after it has
   completed its parse and returned its results.

   The newly parsed sentence will be sent to a scheme function called
   ''check_query_type''.This function will identify the speech act type
   of the sentence then the next appropriate processing functions will
   be called. For instance currently  ''(wh_query_process query)'' will
   be invoked if the speech act tag is 'InterrogativeSpeechAct'

3) Question-Answering. Currently The bot can recognize speech act type
   of the utterance. specifically it identifies 'InterrogativeSpeechAct',
   'TruthQuerySpeechAct and 'DeclarativeSpeechAct'

   The bot can answer some simple wh-questions ('InterrogativeSpeechAct')
   by using the fuzzy-pattern-matcher. Sentences ("facts") pertaining
   to the question must first be present in the atomspace, of course.

4) Relex2Logic replaces the old "semantic triples" code (in the
   ../old-chatbot directory). The current fuzzy-pattern-matcher uses
   the R2L output. Also PLN will use r2l output for reasoning.

5) Sureal and Microplanning is  used for Natural Language Generation.

6) OpenPsi (not hooked up yet) will be used to make the chatbot to
   generate a speech-act goal plus context for utterances. Here,
   different speech act schema will be triggered based on the goal
   and context.

7) Reasoning (not hooked up). PLN, the core reasoning engine will be
   used.

8) Anaphora resolution (not hooked up) will be used to resolve pronouns
   and other references

9) Word-sense disambiguation (not hooked up)
   http://wiki.opencog.org/w/Word_sense_disambiguation


Examples:
---------
Currently, the following dialog is working.


 <Mr. x> hello
 <cogita-bot> Hello Mr. x, you said: "hello
 <cogita-bot> Sorry,I don't know the type

 <Mr. x> Tom eats apples.
 <cogita-bot> Hello Mr. x, you said: "Tom eats apples.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> What does Tom eat?
 <cogita-bot> Hello Mr. x, you said: "What does Tom eat?
 <cogita-bot> You made an Interrogative SpeechAct Tom eats apples .

 <Mr. x> John threw a green ball.
 <cogita-bot> Hello Mr. x, you said: "John threw a green ball.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> who threw a ball?
 <cogita-bot> Hello Mr. x, you said: "who threw a ball?
 <cogita-bot> You made an Interrogative SpeechAct John threw a green ball .

 <Mr. x> Did Fred throw a ball?
 <cogita-bot> Hello Mr. x, you said: "Did Fred throw a ball?
 <cogita-bot> You ask a Truth Query I can't process truth query for now

 <Mr. x> The color of the book is red.
 <cogita-bot> Hello Mr. x, you said: "The color of the book is red.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> What is the color of the book?
 <cogita-bot> Hello Mr. x, you said: "What is the color of the book?
 <cogita-bot> You made an Interrogative SpeechAct the color of the book is red .

 <Mr. x> John Kennedy died.
 <cogita-bot> Hello Mr. x, you said: "John Kennedy died.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> who died?
 <cogita-bot> Hello Mr. x, you said: "who died?
 <cogita-bot> You made an Interrogative SpeechAct John Kennedy die

  <Mr. x> The red ball is under the couch.
 <cogita-bot> Hello Mr. x, you said: "The red ball is under the couch.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> Where is the red ball?
 <cogita-bot> Hello Mr. x, you said: "Where is the red ball?
 <cogita-bot> You made an Interrogative SpeechAct the red ball is under the couch .


ToDo:
-----
* Implement the truth-query processing after the issue on the backward
  chaining is fixed

* Write different  speech  act schema's as it is described on the
  "dialog system" paper

* Create an object holding the name of the user, the chat connection,
  and the 'pseudo-continuation' of the chat status: Create a dialog node
  for each dialog and an utterance node for each utterance.

* Integrate the components described in the architecture part

References
==========
[Katz05] Boris Katz, Gary Borchardt and Sue Felshin
    "Syntactic and Semantic Decomposition Strategies for Question
     Answering from Multiple Resources"
     Proceedings of the AAAI 2005 Workshop on Inference 2005 - aaai.org


http://en.wikipedia.org/wiki/MultiNet

“Initial OpenCog Based Dialogue System Design Document”
an ongoing thesis by Ruiting Lian

Initial draft: April 2009 Linas Vepstas
Updated: September 2009 Linas Vepstas
Updated: April 2010  Vamsi Krishna Davuluri
Updated: July 2015 Rodas Solomon
